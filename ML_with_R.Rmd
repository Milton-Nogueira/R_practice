### In this project a dataset about costs for health insurance was used to practice a few machine learning models in R.


```{r message=FALSE, warning=FALSE}
library(fastDummies)
library(ggplot2)  
library(plotly)
library(hrbrthemes)
library(extrafont) 
library(corrgram)
library(caret)
library(caTools)
library(rpart)
library(forecast)
library(ISLR)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
library(dplyr)
library(ConfusionTableR)
library(tidyr)
library(mlbench)
```


```{r}
#Reading the data

df = read.csv("E:/Usuarios/Documentos/R/Medical Insurance/insurance.csv", na.strings="", stringsAsFactors = T)
head(df)

```

```{r}
summary(df)
```

```{r}
#checking missing data
df[!complete.cases(df),]
```

```{r}
#Checking duplicated rows
#df[!duplicated(df),]
sum(duplicated(df))
```



Exploring the variables



1.Age
```{r}
Counts = table(df$age)
barplot(Counts, main="age", xlab="age",  col = c("blue"))
```


```{r}
boxplot(df$age, col = c("blue"))
```


```{r}
summary(df$age)
```



2.Sex

```{r}
Counts = table(df$sex)
barplot(Counts, main="sex", xlab="sex", col = c("blue"))
```




3.BMI
```{r}
hist(df$bmi, col = c("blue"))
```
```{r}
boxplot(df$bmi,  col = c("blue"))
```
```{r}
summary(df$bmi)
```



4.Children
```{r}
Counts = table(df$children)
barplot(Counts, main="children", xlab="children",col = c("blue"))
```
```{r}
boxplot(df$children,  col = c("blue"))
```
```{r}
summary(df$children)
```



5.Smoker
```{r}
Counts = table(df$smoker)
barplot(Counts, main="smoker", xlab="smoker",col = c("blue"))
```



6.Region
```{r}
Counts = table(df$region)
barplot(Counts, main="region", xlab="region",col = c("blue"))
```



Variable Response - Charges
```{r}
hist(df$charges,col = c("blue"))
```
```{r}
boxplot(df$charges,  col = c("blue"))
```
```{r}
summary(df$charges)
```



We can see that charges don't have a normal distribution, making a log transformation give us a distribution that tends to normal. For this project that transformation will suffice.



```{r}
log_charges = log(df$charges)
hist(log_charges,col = c("blue"))
```
```{r}
# Histogram overlaid with kernel density curve
ggplot(df, aes(x=charges)) + 
    geom_histogram(aes(y=after_stat(density)),    
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666")  
```
```{r}
ggplot(df, aes(x=log(charges))) + 
    geom_histogram(aes(y=after_stat(density)),    
                   colour="black", fill="white") +
    geom_density(alpha=.2, fill="#FF6666")  
```





Let's explore how other variables behave in relation to our response variable.


```{r warning=FALSE}
plot<-ggplot(df, aes(x=bmi, y=charges, color=smoker)) + 
    geom_point(size=2) +
    theme_ipsum() +
    scale_color_manual(values=c("blue", "red"))
print(plot + ggtitle("BMI x Charges"))
```
```{r warning=FALSE}
plot<-ggplot(df, aes(x=age, y=charges, color=smoker)) + 
    geom_point(size=2) +
    theme_ipsum() +
    scale_color_manual(values=c("blue", "red"))
print(plot + ggtitle("Age x Charges"))
```





From those plots we can see that people who smoke have higher costs. It seems that's the biggest correlation with the costs even without checking the correlation. Cost also seems to increase slightly with Age and BMI.
We will check further correlations in the pre-processing step.


### Pre-processing data



```{r}
#Changing yes and no to 0 and 1 in the column smoker
df$smoker<-ifelse(df$smoker=="yes",1,0)
Counts = table(df$smoker)
barplot(Counts, main="smoker", xlab="smoker",  col = c("blue"))
```
```{r}
#One hot encoding on the categorical variables remaining
df <- dummy_cols(df,select_columns = "sex")
df <- dummy_cols(df,select_columns = "region")
```
```{r}
df = subset(df, select = -c(region) )
df = subset(df, select = -c(sex) )
```

```{r}
#Checking the correlation of our variables
cor(cor(df))
corrgram(df, order = TRUE, lower.panel = panel.shade, upper.panel = panel.pie, text.panel = panel.txt, main="Correlation between variables")
```






Smoker variable has the highest correlation with our response variable as we suspected. Age and BMI don't have a strong correlation, but are the next ones with the highest correlation with charges variable.





Next we will apply log in the response variable and make some simple re-scaling in age and BMI.


```{r}
df$charges <- log(df$charges) 
```

```{r}
df$bmi <- (df$bmi)/100
df$age <- (df$age)/100
```


```{r}
#Splitting data into train-test samples
set.seed(42)
split = sample.split(Y=df$charges, SplitRatio=0.8)
train = df[split,]
test = df[!split,]

dim(train)
dim(test)
```





### Multiple Linear Regression




Our first ML analysis is a multiple linear regression to determine the medical costs. 'Smoker', 'Age' and 'BMI' are our independent variables in this scenario.





```{r}
lr_model = lm(charges ~ smoker + age + bmi, data = train)
lr_model
```
```{r}
summary(lr_model)
```
```{r}
Prediction <-predict(lr_model, newdata=test)
results <- data.frame(Actual= test$charges, Prediction)
head(results)
```
```{r}

RSQUARE = function(y_actual,y_predict){
  cor(y_actual,y_predict)^2
}
MAPE = function(y_actual,y_predict){
  mean(abs((y_actual-y_predict)/y_actual))*100
}
```

```{r}
model_R_Squared = RSQUARE(test$charges, Prediction)
model_R_Squared
```
```{r}
model_MAPE = MAPE(test$charges, Prediction)
model_MAPE
```
```{r}
Accuracy_Linear = 100 - model_MAPE
Accuracy_Linear
```

```{r}
plot(Prediction, y= test$charges,
     xlab='Predicted Values',
     ylab='Actual Values',
     main='Predicted vs. Actual Values',col = c("blue"))
abline(a=0, b=1)
```



### Decision Tree

We will use the same variables to predict the charges with a decision tree model.

```{r}
#train
tree_model = rpart(charges ~ smoker + bmi + age, data=train)
```


```{r}
tree_model
summary(tree_model)
```
```{r}
prp(tree_model)
```
```{r}
predic_tree = predict(tree_model, test)
head(predic_tree)
```
```{r}
comp_tree = cbind(predic_tree, test$charges, predic_tree - test$charges)
```
```{r}
head(comp_tree)
```
```{r}
accuracy(predic_tree, test$charges)
```




### Classification with Decision Tree



Now, let's change our point of view. 

People can lie about their smoking habits when filling their register for insurance. This can be configured as fraud since it will generate higher insurance costs. 


Suppose that we already have the medical costs, we want to determine if people smoke or not.



```{r}
tree_model_class = rpart(smoker ~ ., data=train, method="class")
tree_model_class
```
```{r}
predic_tree_class = predict(tree_model_class, test, type ="class")
#predic_tree_class
```
```{r}
rpart.plot(tree_model_class)
prp(tree_model_class)
```

```{r}
rf_class <- predict(tree_model_class, newdata = test, type = "class") 
predictions <- cbind(data.frame(train_preds=rf_class, 
                                test$smoker))
#predictions
```
```{r}
cm <- caret::confusionMatrix(predictions$train_preds, as.factor(predictions$test.smoker))
print(cm) 
```
```{r}
draw_confusion_matrix <- function(cm) {

  layout(matrix(c(1,1,2)))
  par(mar=c(2,2,2,2))
  plot(c(100, 345), c(300, 450), type = "n", xlab="", ylab="", xaxt='n', yaxt='n')
  title('CONFUSION MATRIX', cex.main=2)

  # create the matrix 
  rect(150, 430, 240, 370, col='blue')
  text(195, 435, 'Dont Smoke', cex=1.2)
  rect(250, 430, 340, 370, col='red')
  text(295, 435, 'Smoke', cex=1.2)
  text(125, 370, 'Predicted', cex=1.3, srt=90, font=2)
  text(245, 450, 'Actual', cex=1.3, font=2)
  rect(150, 305, 240, 365, col='red')
  rect(250, 305, 340, 365, col='blue')
  text(140, 400, 'Dont Smoke', cex=1.2, srt=90)
  text(140, 335, 'Smoke', cex=1.2, srt=90)

  # add in the cm results 
  res <- as.numeric(cm$table)
  text(195, 400, res[1], cex=1.6, font=2, col='white')
  text(195, 335, res[2], cex=1.6, font=2, col='white')
  text(295, 400, res[3], cex=1.6, font=2, col='white')
  text(295, 335, res[4], cex=1.6, font=2, col='white')

  # add in the specifics 
  plot(c(100, 0), c(100, 0), type = "n", xlab="", ylab="", main = "Metrics", xaxt='n', yaxt='n')
  text(10, 85, names(cm$byClass[1]), cex=1.2, font=2)
  text(10, 70, round(as.numeric(cm$byClass[1]), 3), cex=1.2)
  text(30, 85, names(cm$byClass[2]), cex=1.2, font=2)
  text(30, 70, round(as.numeric(cm$byClass[2]), 3), cex=1.2)
  text(50, 85, names(cm$byClass[5]), cex=1.2, font=2)
  text(50, 70, round(as.numeric(cm$byClass[5]), 3), cex=1.2)
  text(70, 85, names(cm$byClass[6]), cex=1.2, font=2)
  text(70, 70, round(as.numeric(cm$byClass[6]), 3), cex=1.2)
  text(90, 85, names(cm$byClass[7]), cex=1.2, font=2)
  text(90, 70, round(as.numeric(cm$byClass[7]), 3), cex=1.2)

  # add in the accuracy information 
  text(30, 35, names(cm$overall[1]), cex=1.5, font=2)
  text(30, 20, round(as.numeric(cm$overall[1]), 3), cex=1.4)
  text(70, 35, names(cm$overall[2]), cex=1.5, font=2)
  text(70, 20, round(as.numeric(cm$overall[2]), 3), cex=1.4)
}  
```

```{r}
draw_confusion_matrix(cm)
```


Let's check our False Positives: 

Instead of thinking about it just as a prediction error from the model, we can also look at it as people who stated they don't smoke but our model predicts they do. Considering the strong correlation of the smoker variable with the charges, this could mean financial loss to the company if those people are lying in their register. It sounds reasonable for the company to investigate those cases and similar profiles in the future.




